# === one-cell bootstrap ===
import os, sys, subprocess
root = "/content/ai_rmf_poc"; os.makedirs(root, exist_ok=True)
files = {
"requirements.txt": "chromadb>=0.5.3\nsentence-transformers>=3.0.1\npypdf>=4.2.0\npandas>=2.2.2\n",
"util.py": r'''import os, re
from typing import List, Tuple
from pypdf import PdfReader
def load_pdfs(data_dir: str) -> List[Tuple[str,int,str]]:
    paths, results = [], []
    for r,_,fs in os.walk(data_dir):
        for f in fs:
            if f.lower().endswith(".pdf"): paths.append(os.path.join(r,f))
    for p in sorted(paths):
        try:
            r = PdfReader(p)
            for i, page in enumerate(r.pages):
                try: text = page.extract_text() or ""
                except: text = ""
                if text.strip(): results.append((p, i+1, text))
        except Exception as e:
            print("[WARN] Failed:", p, e)
    return results
def chunk_text(t, chunk_size=1000, overlap=200):
    import re; t = re.sub(r"\s+"," ",t).strip()
    out=[]; s=0; n=len(t)
    while s<n:
        e=min(s+chunk_size,n); out.append(t[s:e])
        if e==n: break
        s=max(0,e-overlap)
    return out
''',
"ingest.py": r'''import os, argparse, uuid, chromadb
from sentence_transformers import SentenceTransformer
from util import load_pdfs, chunk_text
def main(data_dir, db_path, collection="docs", chunk_size=1000, overlap=200, batch=64):
    os.makedirs(db_path, exist_ok=True)
    client = chromadb.PersistentClient(path=db_path)
    coll = client.get_or_create_collection(name=collection)
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    pages = load_pdfs(data_dir)
    if not pages: print("No PDFs in", data_dir); return
    texts=metas=ids=[]
    texts, metas, ids = [], [], []
    for (src, pg, txt) in pages:
        for i, ch in enumerate(chunk_text(txt, chunk_size, overlap)):
            texts.append(ch); metas.append({"source":src,"page":pg}); ids.append(f"{uuid.uuid4().hex}_{pg}_{i}")
            if len(texts)>=batch:
                embs = model.encode(texts, normalize_embeddings=True).tolist()
                coll.add(documents=texts, metadatas=metas, embeddings=embs, ids=ids)
                texts, metas, ids = [], [], []
    if texts:
        embs = model.encode(texts, normalize_embeddings=True).tolist()
        coll.add(documents=texts, metadatas=metas, embeddings=embs, ids=ids)
    print(f"✅ Ingest complete. Count={coll.count()} DB={db_path}")
if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--data_dir", required=True)
    ap.add_argument("--db", default="./chroma_db")
    ap.add_argument("--collection", default="docs")
    ap.add_argument("--chunk_size", type=int, default=1000)
    ap.add_argument("--overlap", type=int, default=200)
    a=ap.parse_args(); main(a.data_dir,a.db,a.collection,a.chunk_size,a.overlap)
''',
"ask_cli.py": r'''import os, argparse, time, chromadb, csv
from sentence_transformers import SentenceTransformer
from datetime import datetime
def log_row(path,row):
    ex=os.path.exists(path)
    with open(path,"a",newline="") as f:
        w=csv.DictWriter(f,fieldnames=["ts","query","top_sources","latency_s","answer_len","model","usage_tokens"])
        if not ex: w.writeheader()
        w.writerow(row)
def main(db_path, collection, query, top_k):
    client=chromadb.PersistentClient(path=db_path)
    coll=client.get_or_create_collection(name=collection)
    embed=SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    q_emb=embed.encode([query], normalize_embeddings=True).tolist()
    t0=time.time()
    res=coll.query(query_embeddings=q_emb, n_results=top_k, include=["documents","metadatas"])
    latency=time.time()-t0
    docs=res["documents"][0] if res["documents"] else []
    metas=res["metadatas"][0] if res["metadatas"] else []
    pairs=[(os.path.basename(m.get("source","")), m.get("page","?"), d) for d,m in zip(docs,metas)]
    if not pairs:
        answer="No relevant excerpts found."
        top_sources=""
    else:
        answer="Top relevant excerpts:\n" + "\n\n".join([f"- ({s}:{p}) {d[:500]}..." for s,p,d in pairs])
        top_sources=";".join([f"{s}:{p}" for s,p,_ in pairs])
    print("\n=== ANSWER ===\n", answer, "\n")
    print("=== SOURCES ==="); [print(f"- {s} (p.{p})") for s,p,_ in pairs]
    os.makedirs("./logs", exist_ok=True)
    log_row("./logs/interactions.csv", {
        "ts": datetime.utcnow().isoformat(),
        "query": query,
        "top_sources": top_sources,
        "latency_s": round(latency,3),
        "answer_len": len(answer),
        "model": "extractive",
        "usage_tokens": ""
    })
if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--db", default="./chroma_db")
    ap.add_argument("--collection", default="docs")
    ap.add_argument("--q", required=True)
    ap.add_argument("--k", type=int, default=4)
    a=ap.parse_args(); main(a.db,a.collection,a.q,a.k)
''',
}
for p,c in files.items():
    with open(f"{root}/{p}","w") as f: f.write(c)
os.makedirs(f"{root}/data", exist_ok=True); os.makedirs(f"{root}/logs", exist_ok=True)
print("Installing deps…")
subprocess.run([sys.executable,"-m","pip","install","-q","-r",f"{root}/requirements.txt"], check=False)
print("✅ Project created at", root)
print("Next:")
print("1) Upload PDFs into /content/ai_rmf_poc/data (left Files panel)")
print("2) Ingest:  !python /content/ai_rmf_poc/ingest.py --data_dir /content/ai_rmf_poc/data --db /content/ai_rmf_poc/chroma_db")
print('3) Ask:     !python /content/ai_rmf_poc/ask_cli.py --db /content/ai_rmf_poc/chroma_db --q "What are the core functions in the NIST AI RMF?"')

